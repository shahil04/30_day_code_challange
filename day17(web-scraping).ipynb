{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEB SCRAPING\n",
    "\n",
    "OBJECTIVES<br>\n",
    "Introduction to Web Scraping<br>\n",
    "Define what web scraping is and the issues surrounding it<br>\n",
    "Use the requests and BeautifulSoup modules to parse HTML<br>\n",
    "Explain some common problems with web scraping<br>\n",
    "Explore other tools that can interact with web pages<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction to Web Scraping\n",
    "\n",
    "Web scraping involves programmatically grabbing data from a web page<br>\n",
    "Three steps: Download, extract data, PROFIT!<br>\n",
    "Okay...more like, do something with data<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why Scrape?\n",
    "\n",
    "There's data on a site that you want to store or analyze<br>\n",
    "You can't get by other means (e.g. an API)<br>\n",
    "You want to programmatically grab the data (instead of lots of manual copying/pasting)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is it...ok?\n",
    "\n",
    "Some websites don't want people scraping them<br>\n",
    "Best practice: consult the robots.txt file\n",
    "If making many requests, time them out<br>\n",
    "If you're too aggressive, your IP can be blocked<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting started with Beautiful Soup\n",
    "\n",
    "To extract data from HTML, we'll use Beautiful Soup\n",
    "Install it with pip\n",
    "Beautiful Soup lets us navigate through HTML with Python\n",
    "Beautiful Soup does NOT download HTML - for this, we need the requests module!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# both are internet responder\n",
    "import requests as r\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"http://www.extracatchy.net/pictures-without-text-with-deep-meaning/\"\n",
    "html_page=r.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "html=urlopen(\"http://www.python.org\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a bs4 object\n",
    "soup=bs(html_page.content,\"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "its work fine\n"
     ]
    }
   ],
   "source": [
    "# if page have some error then we apply excption handling \n",
    "from urllib.error import HTTPError,URLError\n",
    "try:\n",
    "    html_page=r.get(url)\n",
    "except HTTPError as e:\n",
    "    print(e)\n",
    "except URLError as u:\n",
    "    print(\"url not correct\")\n",
    "    \n",
    "else:\n",
    "    print(\"its work fine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#when you found some tag or attribute error such as it is not found\n",
    "# them you also apply a exception handling\n",
    "def get_title(soup):\n",
    "    try:\n",
    "        title=soup.find('title')\n",
    "    except AttributeError as a:\n",
    "        return \"error\"\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>63 Awesome! Pictures (Without Text) With Deep Meanings [2019]</title>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_title(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#soup.findAll(\"table\"[4].findAll(\"tr\")[3].find(\"td\").findAll(\"div\")[1].find(\"a\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#find and findAll functions<br>\n",
    "find(tag,attributes,text,recurcive,limit,keywords)<br>\n",
    "findAll(tag,attributes,text,recurcive,limit,keywords)<br>\n",
    "\n",
    "Tags    =.findAll({\"h1\",\"h2\"})<br>\n",
    "attributes=.findAll(\"span\",{\"class\":{\"red\",\"green\"}})<br>\n",
    "recursive =Boolen<br>\n",
    "text      =.findAll(text=\"name\")<br>\n",
    "limit     =.findAll() with limit of 1<br>\n",
    "keywords  =.findAll(id=\"title\",class=\"image\")<br>\n",
    "kewords example  also =.findAll(\"\",{\"id\":\"green\"})<br>\n",
    "                      =.filndAll(class_=\"image\")<br>\n",
    "                      =.findAll(\"tag/attributes\",{\"class\":\"red\"})<br>\n",
    "                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tag objects- bsobj.div.h1\n",
    "navigablestring objects =text within tags\n",
    "the comment object= <!-a comment object-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Navigating Trees\n",
    "\n",
    "bsobj.tag.subtag.anotherSubTag\n",
    "\n",
    "Children and other descendants\n",
    "<ul>\n",
    "table\n",
    "    <li>tr </li>\n",
    "    <li> th</li>\n",
    "    <li> td</li>\n",
    "    <li> img</li>\n",
    "    <li> span</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## siblings & parents\n",
    "next_siblings() #means you acces tag which is after it\n",
    "previous_siblings  #means you acces tag which is before it\n",
    ".parent/.parents #which is find the parent attrs of seaech tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular expresson\n",
    "\n",
    "use to find a pattern in our page of text\n",
    "for example\n",
    "\n",
    "find all email from your page then<br>\n",
    "we apply regx in pattern<br>\n",
    "pattern=== [A-Za-z0-9\\.+]+@[A-za-z]+\\.(com/edu/org)\n",
    "\n",
    "you can use www.regexpal.com to find pattern\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "commom regex symmbol \n",
    "*        a*b*\n",
    "+       a+b+\n",
    "[]      [A-Z]*\n",
    "()      (a*b)*\n",
    "{m,n}   a{2,3}b{2,3}\n",
    "[^]     [^A-Z]*\n",
    "|        b(a|i|e)c\n",
    ".        b.d\n",
    "^       ^a\n",
    "\\       \\.\\|\\\\\n",
    "$       [A-Z]*[a-z]*$\n",
    "?!      ^((?![A-Z]).)*$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bsobj' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-1b0a95c05eb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbsobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"img\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"src\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\.\\./img\\/gifts/img.*\\.jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'bsobj' is not defined"
     ]
    }
   ],
   "source": [
    "import re\n",
    "bsobj.findall(\"img\",{\"src\":re.compile(\"\\.\\./img\\/gifts/img.*\\.jpg\")})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lambda expressions and web scraping\n",
    "\n",
    "lambda expression =a function that is into anoter function as a variable.\n",
    "functions as parameters into findall,tag objects-->boolen\n",
    "\n",
    "example\n",
    "soup.findAll(lambda tag:len(tag.attrs)==2)\n",
    "###### the its scrap all part of html which have a two attributes\n",
    "eg;-\n",
    "\n",
    "div class=\"body\",id =\"content\"></div><br>\n",
    "span style=\"color:\"red\" class=\"title\"></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing and Navigating HTML\n",
    "\n",
    "BeautifulSoup(html_string, \"html.parser\") - parse HTML\n",
    "Once parsed, There are several ways to navigate:\n",
    "By Tag Name\n",
    "Using find - returns one matching tag\n",
    "Using find_all - returns a list of matching tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-250c900222c2>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-250c900222c2>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Navigating with CSS Selectors\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Navigating with CSS Selectors\n",
    "\n",
    "select - returns a list of elements matching a CSS selector\n",
    "\n",
    "Selector Cheatsheet\n",
    "\n",
    "Select by id of foo: #foo\n",
    "Select by class of bar: .bar\n",
    "Select children: div > p\n",
    "Select descendents: div p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-95b9e8423ad3>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-95b9e8423ad3>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Selecting Elements by Attribute\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Selecting Elements by Attribute\n",
    "\n",
    "# find an element with an id of foo\n",
    "soup.find(id=\"foo\")\n",
    "soup.select(\"#foo\")[0]\n",
    "\n",
    "# find all elements with a class of bar\n",
    "# careful! \"class\" is a reserved word in Python\n",
    "soup.find_all(class_=\"bar\")\n",
    "soup.select(\".bar\")\n",
    "\n",
    "# find all elements with a data\n",
    "# attribute of \"baz\"\n",
    "# using the general attrs kwarg\n",
    "soup.find_all(attrs={\"data-baz\": True})\n",
    "soup.select(\"[data-baz]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing Data in Elements\n",
    "\n",
    "get_text - access the inner text in an element\n",
    "name - tag name\n",
    "attrs - dictionary of attributes\n",
    "You can also access attribute values using brackets!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Navigating with Beautiful Soup\n",
    "\n",
    "Via Tags<br>\n",
    "\n",
    "parent / parents<br>\n",
    "contents<br>\n",
    "next_sibling / next_siblings<br>\n",
    "previous_sibling / previous_siblings<br>\n",
    "Via Searching<br>\n",
    "\n",
    "find_parent / find_parents<br>\n",
    "find_next_sibling / find_next_siblings<br>\n",
    "find_previous_sibling / find_previous_siblings<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requests + Beautiful Soup Example\n",
    "\n",
    "Let's scrape data into a CSV!<br>\n",
    "Goal: Grab all links from Rithm School blog<br>\n",
    "Data: store URL, anchor tag text, and date<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common Issues with Web Scraping\n",
    "\n",
    "Gnarly HTML<br>\n",
    "Code tightly coupled to UI<br>\n",
    "Sanitizing data after grabbing it<br>\n",
    "Data that isn't part of HTML, but is loaded later!<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other Tools\n",
    "\n",
    "Scrapy: https://scrapy.org/\n",
    "Selenium: http://www.seleniumhq.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapy\n",
    "\n",
    "A more streamlined way to build web crawlers, which can programmatically navigate across multiple pages\n",
    "Can export to many different file formats from the command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-f53837321b19>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-f53837321b19>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    Allows you to open up a browser window from your code!\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Selenium\n",
    "\n",
    "Allows you to open up a browser window from your code!<br>\n",
    "Often used with testing<br>\n",
    "Requires a driver for your browser of choice<br>\n",
    "Doesn't navigate through the page until all contents have loaded<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recap\n",
    "\n",
    "Web scraping is the process of downloading, extracting, and storing data from a web page<br>\n",
    "It's helpful when there's no other way to grab data you want<br>\n",
    "Be sure you're allowed to scrape before you do so<br>\n",
    "BeautifulSoup + requests allow you to scrape websites in Python<br>\n",
    "Building scrapers can take time up front, but should save you time in the long term<br>\n",
    "Other helpful tools include Scrapy and Selenium<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
